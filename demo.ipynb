{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0aae418462d4597752470db8b81120da7626953a46a34b943226e1d2a49ace1e6",
   "display_name": "Python 3.7.10 64-bit ('atcs2021': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Learning sentence representations from NLI data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torchtext.legacy import data\n",
    "import torch\n",
    "from torchtext.legacy.datasets import SNLI\n",
    "from torchtext.vocab import GloVe\n",
    "from models import NLITrainer\n",
    "import utils\n",
    "from scipy.spatial import distance\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "CHECKPOINT_PATH = \"./checkpoints\""
   ]
  },
  {
   "source": [
    "## 1 Preparations\n",
    "### 1.1 Load SNLI Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True, \n",
    "                        tokenize=\"spacy\",\n",
    "                        tokenizer_language = 'en_core_web_sm'\n",
    "                        )\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "\n",
    " # make splits for data\n",
    "train, val, test = SNLI.splits(TEXT, LABEL)\n",
    "\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "                                        (train, val, test), \n",
    "                                        batch_size=4, \n",
    "                                        device =\"cpu\")\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, vectors=GloVe(name= '840B', dim= 300))\n",
    "LABEL.build_vocab(train, specials_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9842"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "label_dict ={0:\"entailment\", 1:\"contraction\", 2:\"neutral\"} \n",
    "len(val.examples)"
   ]
  },
  {
   "source": [
    "### 1.2 Load Pretrained Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pretrained_filename:  ./checkpoints/BLSTM_Encoder_Max/lightning_logs/version_7594760/checkpoints/epoch=27-step=120175.ckpt\n",
      "Found pretrained model at ./checkpoints/BLSTM_Encoder_Max/lightning_logs/version_7594760/checkpoints/epoch=27-step=120175.ckpt\n",
      "pretrained_filename:  ./checkpoints/BLSTM_Encoder/lightning_logs/version_7596008/checkpoints/epoch=27-step=120175.ckpt\n",
      "Found pretrained model at ./checkpoints/BLSTM_Encoder/lightning_logs/version_7596008/checkpoints/epoch=27-step=120175.ckpt\n",
      "pretrained_filename:  ./checkpoints/LSTM_Encoder/lightning_logs/version_7594756/checkpoints/epoch=17-step=154511.ckpt\n",
      "Found pretrained model at ./checkpoints/LSTM_Encoder/lightning_logs/version_7594756/checkpoints/epoch=17-step=154511.ckpt\n",
      "pretrained_filename:  ./checkpoints/AWE/lightning_logs/version_7592794/checkpoints/epoch=16-step=145927.ckpt\n",
      "Found pretrained model at ./checkpoints/AWE/lightning_logs/version_7592794/checkpoints/epoch=16-step=145927.ckpt\n"
     ]
    }
   ],
   "source": [
    "BLSTM_Max_trainer = utils.load_latest(NLITrainer, CHECKPOINT_PATH, \"BLSTM_Encoder_Max\", \n",
    "                                inference=True, \n",
    "                                map_location=\"cpu\", silent = False)\n",
    "BLSTM_trainer = utils.load_latest(NLITrainer, CHECKPOINT_PATH, \"BLSTM_Encoder\", \n",
    "                                inference=True, \n",
    "                                map_location=\"cpu\", silent = False)\n",
    "LSTM_trainer = utils.load_latest(NLITrainer, CHECKPOINT_PATH, \"LSTM_Encoder\", \n",
    "                                inference=True, \n",
    "                                map_location=\"cpu\", silent = False)\n",
    "AWE_trainer = utils.load_latest(NLITrainer, CHECKPOINT_PATH, \"AWE\", \n",
    "                                inference=True, \n",
    "                                map_location=\"cpu\", silent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(trainer, hypo, prem):\n",
    "    out = trainer([prem, hypo])\n",
    "    preds = out.argmax(dim=-1)\n",
    "    preds = label_dict[preds.item()]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(trainer, text):\n",
    "    with torch.no_grad():\n",
    "        emb = trainer.encode(text).detach().cpu()\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zsh:1: command not found: tensorboard\n"
     ]
    }
   ],
   "source": []
  },
  {
   "source": [
    "## 2 Error Analysis\n",
    "### 1. Examples in SNLI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['a', 'boy', 'enjoys', 'himself', 'at', 'the', 'pool', '.']\n",
      "['a', 'boy', 'in', 'an', 'innertube', 'in', 'the', 'pool', 'splashing', 'and', 'smiling', '.']\n",
      "entailment\n",
      "AWE: contraction\n",
      "LSTM: neutral\n",
      "BLSTM: neutral\n",
      "BLSTM_Max: entailment\n"
     ]
    }
   ],
   "source": [
    "# only BLSTM_max succeeds\n",
    "i = 5000\n",
    "print(val.examples[i].hypothesis)\n",
    "print(val.examples[i].premise)\n",
    "print(val.examples[i].label)\n",
    "\n",
    "hypo, prem=TEXT.process([val.examples[i].hypothesis]), TEXT.process([val.examples[i].premise])\n",
    "true_label = LABEL.process([val.examples[i].label])\n",
    "print(\"AWE:\", predict(AWE_trainer, hypo, prem))\n",
    "print(\"LSTM:\", predict(LSTM_trainer, hypo, prem))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer, hypo, prem))\n",
    "print(\"BLSTM_Max:\",predict(BLSTM_Max_trainer, hypo, prem))"
   ]
  },
  {
   "source": [
    "### 2.2 Switching word orders\n",
    "#### 2.2.1 Without meaning changes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLSTM: neutral\n",
      "BLSTM: entailment\n",
      "\n",
      "BLSTM\n",
      "tensor([[ 0.0183, -0.1460, -0.0072, -0.0148, -0.0192,  0.0217]]) tensor(0.6207)\n",
      "tensor([[ 0.0208, -0.1000, -0.0021,  0.0066, -0.0103,  0.0046]]) tensor(0.6504)\n",
      "\n",
      "BLSTM_max\n",
      "tensor([[ 0.0703, -0.0250,  0.0285,  0.0112,  0.0003, -0.0023]]) tensor(0.3383)\n",
      "tensor([[ 0.0703, -0.0250,  0.0285,  0.0112, -0.0040, -0.0023]]) tensor(0.3387)\n",
      "\n",
      "LSTM\n",
      "tensor([[ 0.0195,  0.0047, -0.0966, -0.0147, -0.0689,  0.0874]]) tensor(0.6014)\n",
      "tensor([[-0.0114,  0.0068, -0.0945, -0.0634, -0.0743,  0.0779]]) tensor(0.6138)\n",
      "\n",
      "When meaning doesn't change, the embedding of BLSTM_max remains almost the same but BLSTM changes much\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "premise_1 = word_tokenize('a boy at the pool smiling and splashing.')\n",
    "premise_2 = word_tokenize('a boy at the pool splashing and smiling.')\n",
    "\n",
    "\n",
    "prem_1, prem_2 =TEXT.process([premise_1]),TEXT.process([premise_2])\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer, hypo, prem_1))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer, hypo, prem_2))\n",
    "print()\n",
    "b_emb_1 = encode(BLSTM_trainer, prem_1)\n",
    "b_emb_2 = encode(BLSTM_trainer, prem_2)\n",
    "b_emb_0 = encode(BLSTM_trainer, prem)\n",
    "\n",
    "bm_emb_1 = encode(BLSTM_Max_trainer, prem_1)\n",
    "bm_emb_2 = encode(BLSTM_Max_trainer, prem_2)\n",
    "bm_emb_0 = encode(BLSTM_Max_trainer, prem)\n",
    "\n",
    "print(\"BLSTM\")\n",
    "print(b_emb_1[:,10:16], torch.max(b_emb_1))\n",
    "print(b_emb_2[:,10:16], torch.max(b_emb_2))\n",
    "\n",
    "\n",
    "print(\"\\nBLSTM_max\")\n",
    "print(bm_emb_1[:,10:16], torch.max(bm_emb_1))\n",
    "print(bm_emb_2[:,10:16], torch.max(bm_emb_2))\n",
    "\n",
    "print(\"\\nLSTM\")\n",
    "l_emb_1 = encode(LSTM_trainer, prem_1)\n",
    "l_emb_2 = encode(LSTM_trainer, prem_2)\n",
    "print(l_emb_1[:,10:16], torch.max(l_emb_1))\n",
    "print(l_emb_2[:,10:16], torch.max(l_emb_2))\n",
    "\n",
    "print(\"\\nWhen meaning doesn't change, the embedding of BLSTM_max remains almost the same but BLSTM changes much\")"
   ]
  },
  {
   "source": [
    "#### 2.2.2 With meaning changes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AWE: entailment\n",
      "LSTM: entailment\n",
      "BLSTM: entailment\n",
      "BLSTM_Max: contraction\n",
      "\n",
      "BLSTM_Max:\n",
      "tensor([[ 0.0814, -0.0357,  0.0422,  0.0062,  0.0754, -0.0055]])\n",
      "tensor([[ 0.1552, -0.0426,  0.0323,  0.0208,  0.0332,  0.0023]])\n",
      "When meaning changes, embeddings are different\n",
      "\n",
      "BLSTM:\n",
      "tensor([[ 0.0441, -0.0313,  0.0261,  0.0487, -0.0014,  0.0725]])\n",
      "tensor([[ 0.0071,  0.0512, -0.0310,  0.0464,  0.0748,  0.0174]])\n",
      "\n",
      "LSTM:\n",
      "tensor([[ 0.0226,  0.0263,  0.0042, -0.1209,  0.0206,  0.0431]])\n",
      "tensor([[-0.0271,  0.0985, -0.0074, -0.1414,  0.0529,  0.0283]])\n",
      "\n",
      "AWE:\n",
      "tensor([[-0.1496, -0.0945, -0.1279, -0.0653, -0.1515,  0.0153]])\n",
      "tensor([[-0.1496, -0.0945, -0.1279, -0.0653, -0.1515,  0.0153]])\n"
     ]
    }
   ],
   "source": [
    "prem_new = word_tokenize('a man pushes a lady to the ground.')\n",
    "hypo_new = word_tokenize('a lady pushes a man')\n",
    "hypo_new1 = word_tokenize('a man pushes a lady')\n",
    "hypo_new, prem_new=TEXT.process([hypo_new]), TEXT.process([prem_new])\n",
    "hypo_new1 = TEXT.process([hypo_new1])\n",
    "\n",
    "print(\"AWE:\", predict(AWE_trainer, hypo_new, prem_new))\n",
    "print(\"LSTM:\", predict(LSTM_trainer, hypo_new, prem_new))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer,hypo_new, prem_new))\n",
    "print(\"BLSTM_Max:\",predict(BLSTM_Max_trainer, hypo_new, prem_new))\n",
    "\n",
    "print(\"\\nBLSTM_Max:\")\n",
    "bm_emb_n = encode(BLSTM_Max_trainer, hypo_new)\n",
    "bm_emb_n1 = encode(BLSTM_Max_trainer, hypo_new1)\n",
    "print(bm_emb_n[:,10:16])\n",
    "print(bm_emb_n1[:,10:16])\n",
    "print(\"When meaning changes, embeddings are different\")\n",
    "\n",
    "print(\"\\nBLSTM:\")\n",
    "b_emb_n = encode(BLSTM_trainer, hypo_new)\n",
    "b_emb_n1 = encode(BLSTM_trainer, hypo_new1)\n",
    "print(b_emb_n[:,10:16])\n",
    "print(b_emb_n1[:,10:16])\n",
    "\n",
    "print(\"\\nLSTM:\")\n",
    "l_emb_n = encode(LSTM_trainer, hypo_new)\n",
    "l_emb_n1 = encode(LSTM_trainer, hypo_new1)\n",
    "print(l_emb_n[:,10:16])\n",
    "print(l_emb_n1[:,10:16])\n",
    "\n",
    "print(\"\\nAWE:\")\n",
    "a_emb_n = encode(AWE_trainer, hypo_new)\n",
    "a_emb_n1 = encode(AWE_trainer, hypo_new1)\n",
    "print(a_emb_n[:,10:16])\n",
    "print(a_emb_n1[:,10:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### 2.3 Changing non-content words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AWE: entailment\n",
      "AWE: contraction\n",
      "LSTM: entailment\n",
      "LSTM: neutral\n",
      "BLSTM: neutral\n",
      "BLSTM: neutral\n"
     ]
    }
   ],
   "source": [
    "premise_3 = word_tokenize('a boy at the pool smiling.')\n",
    "premise_4 = word_tokenize('a boy in the pool smiling.')\n",
    "prem_3 =TEXT.process([premise_3])\n",
    "prem_4 =TEXT.process([premise_4])\n",
    "\n",
    "print(\"AWE:\",predict(AWE_trainer, hypo, prem_3))\n",
    "print(\"AWE:\",predict(AWE_trainer, hypo, prem_4))\n",
    "\n",
    "print(\"LSTM:\", predict(LSTM_trainer, hypo, prem_3))\n",
    "print(\"LSTM:\", predict(LSTM_trainer, hypo, prem_4))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer, hypo, prem_3))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer, hypo, prem_4))\n",
    "\n",
    "#  AWE is sensitive to preposition&word change\n",
    "awe_emb_3 = encode(AWE_trainer, prem_3)\n",
    "awe_emb_4 = encode(AWE_trainer, prem_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AWE: contraction\n",
      "LSTM: neutral\n",
      "BLSTM: neutral\n",
      "BLSTM_Max: entailment\n",
      "tensor([[ 0.0100,  0.0118, -0.0108,  0.0758,  0.0107]])\n",
      "tensor([[ 0.0059,  0.0118, -0.0350,  0.0996,  0.0418]])\n",
      "tensor([[ 0.0100,  0.0118, -0.0141,  0.0884,  0.0429]])\n"
     ]
    }
   ],
   "source": [
    "premise_5 = word_tokenize('a boy outside the pool smiling.')\n",
    "\n",
    "prem_5 =TEXT.process([premise_5])\n",
    "\n",
    "print(\"AWE:\",predict(AWE_trainer, hypo, prem_5))\n",
    "print(\"LSTM:\", predict(LSTM_trainer, hypo, prem_5))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer, hypo, prem_5))\n",
    "print(\"BLSTM_Max:\",predict(BLSTM_Max_trainer, hypo, prem_5))\n",
    "\n",
    "\n",
    "# BLSTM_Max/classifier is not sensitive to preposition change\n",
    "bm_emb_5 = encode(BLSTM_Max_trainer, prem_4)\n",
    "\n",
    "print(bm_emb_3[:,:5])\n",
    "print(bm_emb_4[:,:5])\n",
    "print(bm_emb_5[:,:5])"
   ]
  },
  {
   "source": [
    "### 2.4 Tense change"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AWE: neutral\n",
      "LSTM: entailment\n",
      "BLSTM: entailment\n",
      "BLSTM_Max: entailment\n",
      "\n",
      "BLSTM_Max:\n",
      "tensor([[-0.0273, -0.0025,  0.0203,  0.0073,  0.0148,  0.0065]])\n",
      "tensor([[-0.0222,  0.0115,  0.0203,  0.0014,  0.0124,  0.0147]])\n",
      "When meaning changes, embeddings are different\n",
      "\n",
      "AWE:\n",
      "tensor([[-0.3294, -0.0216,  0.0055, -0.0576, -0.1400,  0.0085]])\n",
      "tensor([[-0.3646, -0.0683, -0.0871, -0.0401, -0.1054,  0.0184]])\n"
     ]
    }
   ],
   "source": [
    "prem_new = word_tokenize('the girl is in a park')\n",
    "hypo_new = word_tokenize('the girl was in a park ')\n",
    "\n",
    "hypo_new, prem_new=TEXT.process([hypo_new]), TEXT.process([prem_new])\n",
    "hypo_new1 = TEXT.process([hypo_new1])\n",
    "\n",
    "print(\"AWE:\", predict(AWE_trainer, hypo_new, prem_new))\n",
    "print(\"LSTM:\", predict(LSTM_trainer, hypo_new, prem_new))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer,hypo_new, prem_new))\n",
    "print(\"BLSTM_Max:\",predict(BLSTM_Max_trainer, hypo_new, prem_new))\n",
    "\n",
    "print(\"\\nBLSTM_Max:\")\n",
    "bm_emb_n = encode(BLSTM_Max_trainer, hypo_new)\n",
    "bm_emb_n1 = encode(BLSTM_Max_trainer, prem_new )\n",
    "print(bm_emb_n[:,10:16])\n",
    "print(bm_emb_n1[:,10:16])\n",
    "\n",
    "print(\"\\nAWE:\")\n",
    "a_emb_n = encode(AWE_trainer, hypo_new)\n",
    "a_emb_n1 = encode(AWE_trainer, prem_new )\n",
    "print(a_emb_n[:,10:16])\n",
    "print(a_emb_n1[:,10:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AWE: entailment\n",
      "LSTM: entailment\n",
      "BLSTM: entailment\n",
      "BLSTM_Max: entailment\n",
      "\n",
      "BLSTM_Max:\n",
      "tensor([[-0.0495,  0.0070,  0.0203,  0.0018,  0.0700,  0.0789]])\n",
      "tensor([[-0.0530,  0.0347,  0.0203,  0.0023,  0.0055,  0.0651]])\n",
      "\n",
      "AWE:\n",
      "tensor([[-0.2802, -0.0079, -0.0012, -0.0887, -0.0896,  0.0890]])\n",
      "tensor([[-0.2994,  0.0042, -0.0472, -0.0425, -0.0973,  0.1327]])\n"
     ]
    }
   ],
   "source": [
    "prem_new = word_tokenize('the girl plays in a park')\n",
    "hypo_new = word_tokenize('the girl played in a park ')\n",
    "\n",
    "hypo_new, prem_new=TEXT.process([hypo_new]), TEXT.process([prem_new])\n",
    "hypo_new1 = TEXT.process([hypo_new1])\n",
    "\n",
    "print(\"AWE:\", predict(AWE_trainer, hypo_new, prem_new))\n",
    "print(\"LSTM:\", predict(LSTM_trainer, hypo_new, prem_new))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer,hypo_new, prem_new))\n",
    "print(\"BLSTM_Max:\",predict(BLSTM_Max_trainer, hypo_new, prem_new))\n",
    "\n",
    "print(\"\\nBLSTM_Max:\")\n",
    "bm_emb_n = encode(BLSTM_Max_trainer, hypo_new)\n",
    "bm_emb_n1 = encode(BLSTM_Max_trainer, prem_new )\n",
    "print(bm_emb_n[:,10:16])\n",
    "print(bm_emb_n1[:,10:16])\n",
    "\n",
    "print(\"\\nAWE:\")\n",
    "a_emb_n = encode(AWE_trainer, hypo_new)\n",
    "a_emb_n1 = encode(AWE_trainer, prem_new )\n",
    "print(a_emb_n[:,10:16])\n",
    "print(a_emb_n1[:,10:16])"
   ]
  },
  {
   "source": [
    "### 2.5 Objective changes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['two', 'people', 'looking', 'at', 'new', 'york', '.']\n",
      "['two', 'people', 'are', 'looking', 'at', 'something', 'in', 'new', 'york', 'city', '.']\n",
      "neutral\n",
      "AWE: neutral\n",
      "LSTM: entailment\n",
      "BLSTM: entailment\n",
      "BLSTM_Max: entailment\n"
     ]
    }
   ],
   "source": [
    "# only BLSTM_max succeeds\n",
    "i = 1000\n",
    "print(val.examples[i].hypothesis)\n",
    "print(val.examples[i].premise)\n",
    "print(val.examples[i].label)\n",
    "\n",
    "hypo, prem=TEXT.process([val.examples[i].hypothesis]), TEXT.process([val.examples[i].premise])\n",
    "true_label = LABEL.process([val.examples[i].label])\n",
    "print(\"AWE:\", predict(AWE_trainer, hypo, prem))\n",
    "print(\"LSTM:\", predict(LSTM_trainer, hypo, prem))\n",
    "print(\"BLSTM:\", predict(BLSTM_trainer, hypo, prem))\n",
    "print(\"BLSTM_Max:\",predict(BLSTM_Max_trainer, hypo, prem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nBLSTM_Max:\ntensor([[ 0.1048,  0.0470,  0.0035,  0.0246, -0.1057,  0.2409]])\ntensor([[ 0.1048,  0.0470,  0.0035,  0.0327, -0.0938,  0.2474]])\nWhen meaning changes, embeddings are similar\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBLSTM_Max:\")\n",
    "bm_emb_n = encode(BLSTM_Max_trainer, hypo)\n",
    "bm_emb_n1 = encode(BLSTM_Max_trainer, prem)\n",
    "print(bm_emb_n[:,10:16])\n",
    "print(bm_emb_n1[:,10:16])\n",
    "print(\"When meaning changes, embeddings are similar\")"
   ]
  },
  {
   "source": [
    "## Summary:\n",
    "1. BLSTM_Max Encoder outperforms the other encoders in most cases. By using maxpooling, it captures the most salient feature of sentences.This enables more robust sentence representations regardless of minor changes in sentences(e.g. replacing words, changing word orders). However, this can also make the classifier neglacte meaning changes caused by such changes.\n",
    "\n",
    "\n",
    "2. AWE Encoder classifer is very sensitive to word changes and additional information, which makes it only succeed in the SNLI tasks when two sentences have many similar words. But such features enables it to performs better than BLSTM_Max Encoder classifier in some cases. When word orders change, the AWE embeddings remain the same.\n",
    "\n",
    "3. LSTM Encoder and BLSTM Encoder have very similar performance over all tasks. They perform better than AWE encoder models but worse than the BLSTM ones. The encoders capture information changes when modifications are made to the sentences, but this doesn't enable the classifers to make good predictions in some cases. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}